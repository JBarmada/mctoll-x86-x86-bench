# McToll x86 → x86 Lifting and Benchmarking Pipeline

This project provides a complete pipeline to evaluate the correctness and performance of code lifted by the `llvm-mctoll` binary-raising tool. It takes a series of C source code problems, compiles them, lifts the resulting binaries to LLVM IR using McToll, re-compiles the lifted IR, and then runs correctness tests and performance benchmarks against the original code.

## Features

- **Automated Lifting Pipeline:** A Python script (`run_mctoll_pipeline.py`) that automates the full lifting process over a problem set.
- **Flexible Optimization Testing:** Easily modify optimization levels for both original and lifted builds.
- **Automated Benchmarking:** The `benchmark.py` script runs performance tests with configurable repetitions, timeouts, and warm-up filtering.
- **Data Export:** All benchmark results are saved to a structured `.json` file.
- **Visualization Tools:** The `plot_results.py` script generates comparative bar graphs with error bars and runtime histograms.

## Directory Structure

Below is a typical layout of the project from the root directory:

```
.
├── eval/
│   └── problem1/
│       ├── code.c              # The source logic to compile/lift
│       └── test.c              # The test harness with main() and asserts
│
├── mctoll_results_O0-O0/       # Output directory from lifted pipeline
│   └── problem1/
│       ├── code.so             # Compiled shared object (input to McToll)
│       ├── code-dis.ll         # LLVM IR generated by McToll
│       └── problem1_raised_test # Executable recompiled from lifted IR
│
├── original_dynamic_binaries_O0/
│   └── problem1_test           # Baseline compiled binary for benchmarking
│
├── .gitignore
├── run_mctoll_pipeline.py # Main automation script
├── compile_originals.py        # Baseline compiler script for originals
├── benchmark.py                # Benchmarking script
├── plot_results.py             # Graphing and histogram script
└── README.md

````

## How to Use

Run all scripts from the root directory of the project.

### Step 1: Compile the Original Binaries

Creates a baseline set of original compiled programs for benchmarking.

```bash
python3 compile_originals.py
````

You will be prompted to enter the optimization level (e.g., `-O0`, `-O2`). This generates a folder like `original_dynamic_binaries_O0/`.

### Step 2: Run the McToll Lifting Pipeline

This will compile, raise, and recompile all the problems in a given range.

```bash
python3 run_mctoll_pipeline.py
```

It will ask for:

* Optimization levels for both shared object and test compilation
* Start and end problem numbers

The results are saved under a directory like `mctoll_results_O0-O0/`.

### Step 3: Run Performance Benchmarks

This script compares the runtimes of the original and lifted executables.

```bash
python3 benchmark.py
```

It asks for the optimization directories used earlier, then benchmarks each problem, skipping those that timeout. It creates a file like:

```
benchmark_results_O0_vs_O0-O0.json
```

### Step 4: Visualize the Results

Generate a bar chart and optional histogram from the JSON data:

```bash
python3 plot_results.py
```

You’ll be prompted to select your input JSON and output file. This creates a graph like:

```
relative_performance_O0_vs_O0-O0.png
```

## Dependencies

Install the required Python packages using:

pip install matplotlib numpy
